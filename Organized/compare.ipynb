{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1_path = r'D:\\Research\\Thesis\\Organized\\filtered_22-4-2025.csv'\n",
    "df1 = pd.read_csv(file1_path)\n",
    "\n",
    "\n",
    "file2_path = r'D:\\Research\\Thesis\\Organized\\Outputs\\gpt3.5_turbo_user_stories_zero_shot.csv'\n",
    "df2 = pd.read_csv(file2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in filtered_22-4-2025.csv:\n",
      "['data', 'app', 'type', 'user-story']\n",
      "\n",
      "Columns in gpt3.5_turbo_user_stories_zero_shot.csv:\n",
      "['data', 'app', 'type', 'generated_user_story']\n"
     ]
    }
   ],
   "source": [
    "# Print column names of the first CSV\n",
    "print(\"Columns in filtered_22-4-2025.csv:\")\n",
    "print(df1.columns.tolist())\n",
    "\n",
    "# Print column names of the second CSV\n",
    "print(\"\\nColumns in gpt3.5_turbo_user_stories_zero_shot.csv:\")\n",
    "print(df2.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install rouge-score nltk bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shadm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shadm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shadm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.data.path.append(r'C:\\Users\\shadm\\AppData\\Roaming\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the first 100 rows\n",
    "ground_truths = df1['user-story'].fillna('').astype(str).head(100).tolist()\n",
    "predictions = df2['generated_user_story'].fillna('').astype(str).head(100).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scorer_l = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "rouge_l_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, pred in zip(ground_truths, predictions):\n",
    "    # ROUGE-L\n",
    "    rouge_l = rouge_scorer_l.score(ref, pred)['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l)\n",
    "\n",
    "    # BLEU (with smoothing)\n",
    "    ref_tokens = nltk.word_tokenize(ref)\n",
    "    pred_tokens = nltk.word_tokenize(pred)\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth_fn)\n",
    "    bleu_scores.append(bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:25<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 25.39 seconds, 3.94 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = bert_score(predictions, ground_truths, lang=\"en\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for gpt3.5 turbo zero shot prompt\n",
      "Average ROUGE-L Score for: 0.2594\n",
      "Average BLEU Score: 0.0644\n",
      "Average BERTScore F1: 0.8693\n"
     ]
    }
   ],
   "source": [
    "print(f\"for gpt3.5 turbo zero shot prompt\")\n",
    "print(f\"Average ROUGE-L Score for: {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
    "print(f\"Average BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"Average BERTScore F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file3_path = r'D:\\Research\\Thesis\\Organized\\Outputs\\gpt3.5turbo_user_stories_one_shot.csv'\n",
    "df3 = pd.read_csv(file3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = df1['user-story'].fillna('').astype(str).head(100).tolist()\n",
    "predictions = df3['generated_user_story'].fillna('').astype(str).head(100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scorer_l = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "rouge_l_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, pred in zip(ground_truths, predictions):\n",
    "    # ROUGE-L\n",
    "    rouge_l = rouge_scorer_l.score(ref, pred)['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l)\n",
    "\n",
    "    # BLEU (with smoothing)\n",
    "    ref_tokens = nltk.word_tokenize(ref)\n",
    "    pred_tokens = nltk.word_tokenize(pred)\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth_fn)\n",
    "    bleu_scores.append(bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "100%|██████████| 2/2 [00:00<00:00, 104.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 12.37 seconds, 8.08 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = bert_score(predictions, ground_truths, lang=\"en\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for gpt3.5 turbo one shot prompt\n",
      "Average ROUGE-L Score for: 0.4697\n",
      "Average BLEU Score: 0.2232\n",
      "Average BERTScore F1: 0.8933\n"
     ]
    }
   ],
   "source": [
    "print(f\"for gpt3.5 turbo one shot prompt\")\n",
    "print(f\"Average ROUGE-L Score for: {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
    "print(f\"Average BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"Average BERTScore F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
