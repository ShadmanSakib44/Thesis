{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_server(prompt, model):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            complete_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        chunk = json.loads(line)\n",
    "                        if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                            complete_response += chunk[\"message\"][\"content\"]\n",
    "                    except (KeyError, json.JSONDecodeError):\n",
    "                        continue\n",
    "            return complete_response\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error connecting to the server:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompts(input_file, output_file, models, prompts, max_reviews=100):\n",
    "    # Read the input data\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    if 'review_content' not in df.columns:\n",
    "        print(\"Error: 'review_content' column not found in the input file.\")\n",
    "        return None  # Return None in case of errors\n",
    "\n",
    "    # Limit the dataset to the first `max_reviews` reviews\n",
    "    df = df.head(max_reviews)\n",
    "\n",
    "    # Initialize a results list\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each model and prompt combination\n",
    "    for model in models:\n",
    "        for prompt_template in prompts:\n",
    "            print(f\"Evaluating model: {model} with prompt: {prompt_template[:50]}...\")\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                review_content = row['review_content']\n",
    "                prompt = prompt_template.format(review=review_content)\n",
    "\n",
    "                # Get the response from the model\n",
    "                response = get_response_from_server(prompt, model)\n",
    "\n",
    "                # Append results\n",
    "                results.append({\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": prompt_template,\n",
    "                    \"review_index\": index,\n",
    "                    \"review_content\": review_content,\n",
    "                    \"response\": response\n",
    "                })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"cleaned_sampled_reviews.csv\"\n",
    "output_file = \"prompt_evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma2:27b\", \"codellama:latest\", \"llama3.1:latest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Turn the following review into a concise user story: {review}\",\n",
    "    \"Based on the review below, create a user story in the format: As a [User Role], I want [Goal] so that [Benefit]: {review}\",\n",
    "    \"Generate a user story that summarizes the key features or improvements requested in this review: {review}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemma2:27b with prompt: Turn the following review into a concise user stor...\n",
      "Evaluating model: gemma2:27b with prompt: Based on the review below, create a user story in ...\n",
      "Evaluating model: gemma2:27b with prompt: Generate a user story that summarizes the key feat...\n",
      "Evaluating model: codellama:latest with prompt: Turn the following review into a concise user stor...\n",
      "Evaluating model: codellama:latest with prompt: Based on the review below, create a user story in ...\n",
      "Evaluating model: codellama:latest with prompt: Generate a user story that summarizes the key feat...\n",
      "Evaluating model: llama3.1:latest with prompt: Turn the following review into a concise user stor...\n",
      "Evaluating model: llama3.1:latest with prompt: Based on the review below, create a user story in ...\n",
      "Evaluating model: llama3.1:latest with prompt: Generate a user story that summarizes the key feat...\n",
      "Evaluation results saved to prompt_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_prompts(input_file, output_file, models, prompts, max_reviews=100)\n",
    "\n",
    "if results:  # Only proceed if results are returned\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation results saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the column names \n",
    "print(results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model                                             prompt  \\\n",
      "300  codellama:latest  Turn the following review into a concise user ...   \n",
      "301  codellama:latest  Turn the following review into a concise user ...   \n",
      "302  codellama:latest  Turn the following review into a concise user ...   \n",
      "303  codellama:latest  Turn the following review into a concise user ...   \n",
      "304  codellama:latest  Turn the following review into a concise user ...   \n",
      "\n",
      "     review_index                                     review_content  \\\n",
      "300             0  In addition to being notorious for it39s priva...   \n",
      "301             1  Ive been trying to get a hold of Facebook beca...   \n",
      "302             2  Keep getting banned on Facebook for random thi...   \n",
      "303             3  I have been locked out of my account for 2 wee...   \n",
      "304             4  For all the good Meta does in keeping us conne...   \n",
      "\n",
      "                                              response  \n",
      "300  As a user of Facebook, I want to be able to sh...  \n",
      "301  \\nAs a user, I want to be able to access my Fa...  \n",
      "302  \\nUser Story: As a British user of Facebook, I...  \n",
      "303  \\nUser Story:\\nAs a customer of Instagram and ...  \n",
      "304  \\nAs a user who has experienced significant ha...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"D:\\Research\\Thesis\\prompt_evaluation_results.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the desired prompt order\n",
    "prompt_order = [\n",
    "    \"Turn the following review into a concise user story: {review}\",\n",
    "    \"Based on the review below, create a user story in the format: As a [User Role], I want [Goal] so that [Benefit]: {review}\",\n",
    "    \"Generate a user story that summarizes the key features or improvements requested in this review: {review}\"\n",
    "]\n",
    "\n",
    "# Function to map prompts to their order\n",
    "def get_prompt_order(prompt):\n",
    "    return prompt_order.index(prompt) if prompt in prompt_order else -1\n",
    "\n",
    "# Create a new column 'prompt_order' to hold the order index for each prompt\n",
    "df['prompt_order'] = df['prompt'].apply(get_prompt_order)\n",
    "\n",
    "# Sort the DataFrame by 'model' first and then by 'prompt_order' to maintain the correct order within each model\n",
    "df_sorted = df.sort_values(by=['model', 'prompt_order'])\n",
    "\n",
    "# Drop the 'prompt_order' column as it's no longer needed\n",
    "df_sorted = df_sorted.drop(columns=['prompt_order'])\n",
    "\n",
    "# Save the sorted dataframe to a new CSV file\n",
    "df_sorted.to_csv(r\"D:\\Research\\Thesis\\sorted_prompt_evaluation_results.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the sorted dataframe\n",
    "print(df_sorted.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
