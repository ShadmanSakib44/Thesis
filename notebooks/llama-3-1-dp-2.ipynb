{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9197834,"sourceType":"datasetVersion","datasetId":5560773},{"sourceId":9257608,"sourceType":"datasetVersion","datasetId":5601215},{"sourceId":81878,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from time import time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nfrom IPython.display import display, Markdown","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:55:42.308803Z","iopub.execute_input":"2024-08-27T12:55:42.309218Z","iopub.status.idle":"2024-08-27T12:55:42.314460Z","shell.execute_reply.started":"2024-08-27T12:55:42.309176Z","shell.execute_reply":"2024-08-27T12:55:42.313476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:55:45.086851Z","iopub.execute_input":"2024-08-27T12:55:45.087858Z","iopub.status.idle":"2024-08-27T12:55:58.021320Z","shell.execute_reply.started":"2024-08-27T12:55:45.087804Z","shell.execute_reply":"2024-08-27T12:55:58.020077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport re\nimport time\nimport traceback\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:56:00.053158Z","iopub.execute_input":"2024-08-27T12:56:00.053886Z","iopub.status.idle":"2024-08-27T12:56:00.727104Z","shell.execute_reply.started":"2024-08-27T12:56:00.053844Z","shell.execute_reply":"2024-08-27T12:56:00.726117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3.1/transformers/8b/1\"\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:56:02.933953Z","iopub.execute_input":"2024-08-27T12:56:02.934839Z","iopub.status.idle":"2024-08-27T12:57:49.018683Z","shell.execute_reply.started":"2024-08-27T12:56:02.934797Z","shell.execute_reply":"2024-08-27T12:57:49.017689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    do_sample=True,          \n    temperature=0.3,        \n    top_p=0.85,              \n    max_new_tokens=50,       \n    pad_token_id=tokenizer.eos_token_id,  \n    eos_token_id=tokenizer.eos_token_id   \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:58:18.774372Z","iopub.execute_input":"2024-08-27T12:58:18.775314Z","iopub.status.idle":"2024-08-27T12:58:18.780588Z","shell.execute_reply.started":"2024-08-27T12:58:18.775271Z","shell.execute_reply":"2024-08-27T12:58:18.779649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_file = '/kaggle/input/javafull/java.jsonl'  \noutput_file = '/kaggle/working/javaOutputLLamav_FULL.jsonl'","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:14.989881Z","iopub.execute_input":"2024-08-27T12:59:14.990681Z","iopub.status.idle":"2024-08-27T12:59:14.994869Z","shell.execute_reply.started":"2024-08-27T12:59:14.990641Z","shell.execute_reply":"2024-08-27T12:59:14.993899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(input_file, 'r', encoding='UTF-8') as f:\n    json_data = f.readlines()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:17.122867Z","iopub.execute_input":"2024-08-27T12:59:17.123522Z","iopub.status.idle":"2024-08-27T12:59:17.162231Z","shell.execute_reply.started":"2024-08-27T12:59:17.123483Z","shell.execute_reply":"2024-08-27T12:59:17.161349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_model_output(output):\n  \n   \n    if \"Commit message:\" in output:\n        output = output.split(\"Commit message:\")[-1].strip()\n    \n    # Optionally clean any quotes or unexpected formatting\n    start = output.find('\\\"')\n    end = output.rfind('\\\"')\n    if start != -1 and end != -1:\n        output = output[start+1:end].strip()\n\n    # Ensure the output is clean and concise\n    if \".\" in output:\n        output = output.split(\".\")[0] + \".\"\n\n    return output.strip()  # Final cleanup\n\ndef query_model(diff):\n    \"\"\"Query the model with specific parameters to control the output.\"\"\"\n    try:\n        prompt = (\n            f\"{diff}\\n\"\n            \"###\\n\"\n            \"Commit message:\"\n        )\n\n        response = nlp_pipeline(\n            prompt,\n            max_new_tokens=30, \n            stop_sequence=[\"###\", \"\\n\"],  \n            temperature=0.2 \n        )[0]['generated_text']\n\n     \n        msgGPT = clean_model_output(response)\n\n        return msgGPT\n    except Exception as e:\n        print(\"Error:\", str(e))\n        traceback.print_exc()\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:19.179449Z","iopub.execute_input":"2024-08-27T12:59:19.179872Z","iopub.status.idle":"2024-08-27T12:59:19.188810Z","shell.execute_reply.started":"2024-08-27T12:59:19.179833Z","shell.execute_reply":"2024-08-27T12:59:19.187821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_meteor(sentence1, sentence2):\n    vectorizer = CountVectorizer().fit([sentence1, sentence2])\n    sentence1_vector = vectorizer.transform([sentence1])\n    sentence2_vector = vectorizer.transform([sentence2])\n    similarity = cosine_similarity(sentence1_vector, sentence2_vector)[0][0]\n    score = 2 * similarity * len(sentence1) * len(sentence2) / (len(sentence1) + len(sentence2))\n    return score\n\ndef calculate_bleu(reference, translation):\n    bleu_score = sentence_bleu([reference], translation)\n    return bleu_score\n\ndef calculate_rouge_l(reference, translation):\n    rouge = Rouge()\n    rouge_l_score = rouge.get_scores(translation, reference, avg=True)['rouge-l']\n    return rouge_l_score","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:21.649657Z","iopub.execute_input":"2024-08-27T12:59:21.650453Z","iopub.status.idle":"2024-08-27T12:59:21.657831Z","shell.execute_reply.started":"2024-08-27T12:59:21.650413Z","shell.execute_reply":"2024-08-27T12:59:21.656685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_camel_case(s):\n    return s != s.lower() and s != s.upper() and \"_\" not in s\n\ndef to_Underline(x):\n    return re.sub('(?<=[a-z])[A-Z]|(?<!^)[A-Z](?=[a-z])', ' \\g<0>', x).lower()\n\ndef get_tokens(text):\n    tokens = nltk.word_tokenize(text)\n    if len(tokens) > 1024:\n        return ' '.join(tokens[:1024])\n    else:\n        return ' '.join(tokens)\n\ndef remove_between_identifiers(text, identifier_start, identifier_end):\n    pattern = f'(?<={identifier_start}).*?(?={identifier_end})'\n    result = re.sub(pattern, '', text)\n    result = result.replace(' . ', '.').replace('  ', ' ').replace(' = ', '=').replace(' ; ', ';')\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:23.653085Z","iopub.execute_input":"2024-08-27T12:59:23.653456Z","iopub.status.idle":"2024-08-27T12:59:23.661081Z","shell.execute_reply.started":"2024-08-27T12:59:23.653423Z","shell.execute_reply":"2024-08-27T12:59:23.660013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_data = {\"diff_id\": 0, \"msg\": \"0\", \"msgGPT\": \"0\", \"METEOR Score\": \"0\", \"BLEU Score\": \"0\", \"ROUGE-L Score\": \"0\"}\nwith open(output_file, 'a', encoding='UTF-8') as f:\n    json.dump(initial_data, f)\n    f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:26.020832Z","iopub.execute_input":"2024-08-27T12:59:26.021234Z","iopub.status.idle":"2024-08-27T12:59:26.027162Z","shell.execute_reply.started":"2024-08-27T12:59:26.021196Z","shell.execute_reply":"2024-08-27T12:59:26.026129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor item in json_data:\n    attempts = 0\n    while attempts < 3:\n        try:\n            data = json.loads(item)\n            diff_id = data['diff_id']\n            diff = data['diff']\n\n           \n            result = remove_between_identifiers(diff, 'mmm a', '<nl>')\n            diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\n            msg = data['msg']\n\n           \n            words = msg.split()\n            msg_list = [to_Underline(word) if is_camel_case(word) else word for word in words]\n            msg = ' '.join(msg_list)\n\n           \n            raw_output = query_model(diff)\n            if raw_output is None:\n                attempts += 1\n                continue\n\n           \n            msgGPT = clean_model_output(raw_output)\n\n            \n            bleu_score = calculate_bleu(msg, msgGPT)\n            rouge_l_score = calculate_rouge_l(msg, msgGPT)\n            meteor_score = calculate_meteor(msg, msgGPT)\n\n        \n            merged_dict = {\n                \"diff_id\": diff_id,\n                \"msg\": msg,\n                \"msgGPT\": msgGPT,\n                \"METEOR Score\": meteor_score,\n                \"BLEU Score\": bleu_score,\n                \"ROUGE-L Score\": rouge_l_score['f']\n            }\n\n            with open(output_file, 'a', encoding='UTF-8') as f:\n                json.dump(merged_dict, f)\n                f.write('\\n')\n\n            \n            time.sleep(5)\n            break\n\n        except Exception as e:\n            print(f\"Error processing diff_id {diff_id}: {str(e)}\")\n            traceback.print_exc()\n            attempts += 1\n            if attempts == 3:\n                print(f\"Failed to process item after 3 attempts: {item}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:34.666001Z","iopub.execute_input":"2024-08-27T12:59:34.666680Z","iopub.status.idle":"2024-08-27T16:17:13.989441Z","shell.execute_reply.started":"2024-08-27T12:59:34.666637Z","shell.execute_reply":"2024-08-27T16:17:13.988400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor item in json_data:\n    attempts = 0\n    while attempts < 3:\n        try:\n            data = json.loads(item)\n            diff_id = data['diff_id']\n            diff = data['diff']\n\n          \n            result = remove_between_identifiers(diff, 'mmm a', '<nl>')\n            diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\n            msg = data['msg']\n\n           \n            words = msg.split()\n            msg_list = [to_Underline(word) if is_camel_case(word) else word for word in words]\n            msg = ' '.join(msg_list)\n\n          \n            raw_output = query_model(diff)\n            if raw_output is None:\n                attempts += 1\n                continue\n\n            \n            msgGPT = clean_model_output(raw_output)\n\n           \n            bleu_score = calculate_bleu(msg, msgGPT)\n            rouge_l_score = calculate_rouge_l(msg, msgGPT)\n            meteor_score = calculate_meteor(msg, msgGPT)\n\n          \n            merged_dict = {\n                \"diff_id\": diff_id,\n                \"msg\": msg,\n                \"msgGPT\": msgGPT,\n                \"METEOR Score\": meteor_score,\n                \"BLEU Score\": bleu_score,\n                \"ROUGE-L Score\": rouge_l_score['f']\n            }\n\n            with open(output_file, 'a', encoding='UTF-8') as f:\n                json.dump(merged_dict, f)\n                f.write('\\n')\n\n           \n            time.sleep(5)\n            break\n\n        except Exception as e:\n            print(f\"Error processing diff_id {diff_id}: {str(e)}\")\n            traceback.print_exc()\n            attempts += 1\n            if attempts == 3:\n                print(f\"Failed to process item after 3 attempts: {item}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:49:53.623121Z","iopub.execute_input":"2024-08-27T12:49:53.623559Z","iopub.status.idle":"2024-08-27T12:50:30.999093Z","shell.execute_reply.started":"2024-08-27T12:49:53.623519Z","shell.execute_reply":"2024-08-27T12:50:30.998024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in json_data:\n    attempts = 0\n    while attempts < 3:\n        try:\n           \n            data = json.loads(item)\n            diff_id = data['diff_id']\n            diff = data['diff']\n            \n            \n            result = remove_between_identifiers(diff, 'mmm a', '<nl>')\n            diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\n            msg = data['msg']\n\n          \n            words = msg.split()\n            msg_list = [to_Underline(word) if is_camel_case(word) else word for word in words]\n            msg = ' '.join(msg_list)\n\n           \n            prompt = (\n                f\"{diff}\\n\"\n                \"###\\n\"\n                \"Please write a commit message that summarizes the code change above in a single sentence. \"\n                \"The message should be concise, clear, and avoid general statements.\"\n            )\n\n          \n            response = nlp_pipeline(\n                prompt,\n                max_new_tokens=50\n            )[0]['generated_text']\n\n           \n            msgGPT = response.strip().split('\\n')[-1].strip()\n\n          \n            bleu_score = round(calculate_bleu(msg, msgGPT), 2)\n            rouge_l_score = round(calculate_rouge_l(msg, msgGPT)['f'], 2)\n            meteor_score = round(calculate_meteor(msg, msgGPT), 2)\n\n           \n            merged_data = {\n                \"diff_id\": diff_id,\n                \"msg\": msg,\n                \"msgGPT\": msgGPT,\n                \"METEOR Score\": f\"{meteor_score}\",\n                \"BLEU Score\": f\"{bleu_score}\",\n                \"ROUGE-L Score\": f\"{rouge_l_score}\"\n            }\n\n           \n            with open(output_file, 'a', encoding='UTF-8') as f:\n                json.dump(merged_data, f)\n                f.write('\\n')\n                \n           \n            time.sleep(2)\n            break\n\n        except Exception as e:\n            traceback.print_exc()\n            attempts += 1\n            if attempts == 3:\n                print(f\"Failed to process item after 3 attempts: {item}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:35:56.972080Z","iopub.execute_input":"2024-08-27T12:35:56.973034Z","iopub.status.idle":"2024-08-27T12:36:45.068263Z","shell.execute_reply.started":"2024-08-27T12:35:56.972989Z","shell.execute_reply":"2024-08-27T12:36:45.067150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_data = {\n    \"diff_id\": 44,\n    \"repo\": \"oracle/graal\\n\",\n    \"sha\": \"80c597d0510090d7b278ff0890db3ce303776f5f\\n\",\n    \"time\": \"2020-02-14T16:11:58Z\\n\",\n    \"diff\": \"mmm a / src / com . oracle . truffle . espresso / src / com / oracle / truffle / espresso / nodes / EspressoRootNode . java <nl> ppp b / src / com . oracle . truffle . espresso / src / com / oracle / truffle / espresso / nodes / EspressoRootNode . java <nl> public Object execute ( VirtualFrame frame ) { <nl> BytecodeNode bytecodeNode = getBytecodeNode ( ) ; <nl> bytecodeNode . methodMonitorEnter ( frame , monitor ) ; <nl> } else { <nl> + / / TODO ( Gregersen ) - register monitors on frames for non - bytecode methods <nl> InterpreterToVM . monitorEnter ( monitor ) ; <nl> } <nl> Object result ; <nl> public Object execute ( VirtualFrame frame ) { <nl> getBytecodeNode ( ) . monitorExit ( frame , monitor ) ; <nl> } else { <nl> + / / TODO ( Gregersen ) - exit monitors on frames for non - bytecode methods <nl> InterpreterToVM . monitorExit ( monitor ) ; <nl> } <nl> } <nl>\\n\",\n    \"msg\": \"Add a few todos for implementing monitor lookup ownership on native method frames\\n\"\n}\n\n\nresult = remove_between_identifiers(test_data['diff'], 'mmm a', '<nl>')\nprocessed_diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\nmsg = test_data['msg']\n\n\nprompt = (\n    f\"Code change:\\n{processed_diff}\\n\"\n    \"Commit message:\"\n)\n\nresponse = nlp_pipeline(\n    prompt,\n    max_new_tokens=50\n)[0]['generated_text']\n\nmsgGPT = response.strip().split('\\n')[-1].strip()\n\n\nprint(f\"Original Message: {msg}\")\nprint(f\"Generated Message: {msgGPT}\")\n\n\nbleu_score = round(calculate_bleu(msg, msgGPT), 2)\nrouge_l_score = round(calculate_rouge_l(msg, msgGPT)['f'], 2)\nmeteor_score = round(calculate_meteor(msg, msgGPT), 2)\n\nprint(f\"METEOR Score: {meteor_score}\")\nprint(f\"BLEU Score: {bleu_score}\")\nprint(f\"ROUGE-L Score: {rouge_l_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:40:55.352558Z","iopub.execute_input":"2024-08-27T12:40:55.353014Z","iopub.status.idle":"2024-08-27T12:41:26.717925Z","shell.execute_reply.started":"2024-08-27T12:40:55.352978Z","shell.execute_reply":"2024-08-27T12:41:26.716894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lan = '/kaggle/input/java-dataset/java.jsonl'\nwith open(lan, 'r', encoding='UTF-8') as f:\n    json_data = f.readlines()\n\nout_filename = '/kaggle/working/java_result_13B_llama_3.jsonl'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}