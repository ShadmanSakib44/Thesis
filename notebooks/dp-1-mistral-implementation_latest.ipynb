{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9197834,"sourceType":"datasetVersion","datasetId":5560773}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shadman200042144/dp-2-mistral-implementation?scriptVersionId=194517176\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%capture\n!pip install langchain-community langchain-core\n!pip install -U bitsandbytes\n!pip install accelerate\n!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:00:42.625022Z","iopub.execute_input":"2024-08-29T06:00:42.625336Z","iopub.status.idle":"2024-08-29T06:01:43.284661Z","shell.execute_reply.started":"2024-08-29T06:00:42.625309Z","shell.execute_reply":"2024-08-29T06:01:43.283493Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\nfrom langchain import HuggingFacePipeline\nfrom langchain import PromptTemplate, LLMChain\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:03:19.791907Z","iopub.execute_input":"2024-08-29T06:03:19.79233Z","iopub.status.idle":"2024-08-29T06:03:38.226752Z","shell.execute_reply.started":"2024-08-29T06:03:19.792285Z","shell.execute_reply":"2024-08-29T06:03:38.225813Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-29 06:03:27.538362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-29 06:03:27.538467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-29 06:03:27.672501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import huggingface_hub\nhuggingface_hub.login()\n#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","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:03:38.228479Z","iopub.execute_input":"2024-08-29T06:03:38.229166Z","iopub.status.idle":"2024-08-29T06:03:38.257725Z","shell.execute_reply.started":"2024-08-29T06:03:38.229133Z","shell.execute_reply":"2024-08-29T06:03:38.25641Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c82fe464f064949b8da9ce1c4040a41"}},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:03:43.49254Z","iopub.execute_input":"2024-08-29T06:03:43.492928Z","iopub.status.idle":"2024-08-29T06:03:44.408264Z","shell.execute_reply.started":"2024-08-29T06:03:43.492898Z","shell.execute_reply":"2024-08-29T06:03:44.407383Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nimport json\nimport re\nimport time\nimport traceback\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:03:46.96069Z","iopub.execute_input":"2024-08-29T06:03:46.961018Z","iopub.status.idle":"2024-08-29T06:03:46.968479Z","shell.execute_reply.started":"2024-08-29T06:03:46.960992Z","shell.execute_reply":"2024-08-29T06:03:46.967454Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# Configure BitsAndBytes with CPU offloading for specific parts of the model\nbnb_config = BitsAndBytesConfig(\n    llm_int8_enable_fp32_cpu_offload=True,  # Enable FP32 offload to CPU\n    load_in_8bit=True,  # Enable 8-bit quantization\n    llm_int8_threshold=6.0  # Default threshold for mixed precision\n)\n# Load the Mistral 7B model and tokenizer with a custom device map\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Automatically distribute model layers across devices\n)\n\n# Create a pipeline for text generation\nnlp_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:03:49.245045Z","iopub.execute_input":"2024-08-29T06:03:49.245412Z","iopub.status.idle":"2024-08-29T06:05:10.724412Z","shell.execute_reply.started":"2024-08-29T06:03:49.24538Z","shell.execute_reply":"2024-08-29T06:05:10.723514Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9775116a4d6746d4883ee0bfa00f78b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d9b0c8559c4b74a37bbf0e02b04217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0b661e3e27445e9fa39015e71806c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f6ddddc8e8421885cefffa0497aafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757b070a88844742bc532c46ec1e5af9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3d054da7b544fb8e173499f7245e16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5cd27c8e27b44ae92281967243da7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20e164bc10141d2a4ed67b182101aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce73fcdb0794e04ac65898bf3d81591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408becc4c2314fe4a5845c6508c25a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7d547d196e448b9371ffeeaac68fc3"}},"metadata":{}}]},{"cell_type":"code","source":"# Functions for evaluation metrics\ndef calculate_meteor(sentence1, sentence2):\n    vectorizer = CountVectorizer().fit([sentence1, sentence2])\n    sentence1_vector = vectorizer.transform([sentence1])\n    sentence2_vector = vectorizer.transform([sentence2])\n    similarity = cosine_similarity(sentence1_vector, sentence2_vector)[0][0]\n    score = 2 * similarity * len(sentence1) * len(sentence2) / (len(sentence1) + len(sentence2))\n    return score\n\ndef calculate_bleu(reference, translation):\n    bleu_score = sentence_bleu([reference], translation)\n    return bleu_score\n\ndef calculate_rouge_l(reference, translation):\n    rouge = Rouge()\n    rouge_l_score = rouge.get_scores(translation, reference, avg=True)['rouge-l']\n    return rouge_l_score","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:05:21.046914Z","iopub.execute_input":"2024-08-29T06:05:21.047255Z","iopub.status.idle":"2024-08-29T06:05:21.054424Z","shell.execute_reply.started":"2024-08-29T06:05:21.047216Z","shell.execute_reply":"2024-08-29T06:05:21.053522Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Functions for evaluation metrics\ndef calculate_meteor(sentence1, sentence2):\n    vectorizer = CountVectorizer().fit([sentence1, sentence2])\n    sentence1_vector = vectorizer.transform([sentence1])\n    sentence2_vector = vectorizer.transform([sentence2])\n    similarity = cosine_similarity(sentence1_vector, sentence2_vector)[0][0]\n    score = 2 * similarity * len(sentence1) * len(sentence2) / (len(sentence1) + len(sentence2))\n    return score\n\ndef calculate_bleu(reference, translation):\n    bleu_score = sentence_bleu([reference], translation)\n    return bleu_score\n\ndef calculate_rouge_l(reference, translation):\n    rouge = Rouge()\n    rouge_l_score = rouge.get_scores(translation, reference, avg=True)['rouge-l']\n    return rouge_l_score","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:05:22.613008Z","iopub.execute_input":"2024-08-29T06:05:22.613632Z","iopub.status.idle":"2024-08-29T06:05:22.701109Z","shell.execute_reply.started":"2024-08-29T06:05:22.613597Z","shell.execute_reply":"2024-08-29T06:05:22.700357Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# Preprocessing functions\ndef is_camel_case(s):\n    return s != s.lower() and s != s.upper() and \"_\" not in s\n\ndef to_Underline(x):\n    return re.sub('(?<=[a-z])[A-Z]|(?<!^)[A-Z](?=[a-z])', ' \\g<0>', x).lower()\n\ndef get_tokens(text):\n    tokens = nltk.word_tokenize(text)\n    if len(tokens) > 1024:\n        return ' '.join(tokens[:1024])\n    else:\n        return ' '.join(tokens)\n\ndef remove_between_identifiers(text, identifier_start, identifier_end):\n    pattern = f'(?<={identifier_start}).*?(?={identifier_end})'\n    result = re.sub(pattern, '', text)\n    result = result.replace(' . ', '.').replace('  ', ' ').replace(' = ', '=').replace(' ; ', ';')\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:05:24.402895Z","iopub.execute_input":"2024-08-29T06:05:24.403504Z","iopub.status.idle":"2024-08-29T06:05:24.410554Z","shell.execute_reply.started":"2024-08-29T06:05:24.403472Z","shell.execute_reply":"2024-08-29T06:05:24.40967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define file paths\ninput_file = '/kaggle/input/java-dataset/java.jsonl'  # Your dataset\noutput_file = '/kaggle/working/javaOutput.jsonl' ","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:05:26.785915Z","iopub.execute_input":"2024-08-29T06:05:26.786504Z","iopub.status.idle":"2024-08-29T06:05:26.790454Z","shell.execute_reply.started":"2024-08-29T06:05:26.78647Z","shell.execute_reply":"2024-08-29T06:05:26.789553Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Initialize results file\ninitial_data = {\"diff_id\": 0, \"msg\": \"0\", \"msgGPT\": \"0\", \"METEOR Score\": \"0\", \"BLEU Score\": \"0\", \"ROUGE-L Score\": \"0\"}\nwith open(output_file, 'a', encoding='UTF-8') as f:\n    json.dump(initial_data, f)\n    f.write('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:05:27.90447Z","iopub.execute_input":"2024-08-29T06:05:27.90483Z","iopub.status.idle":"2024-08-29T06:05:27.910465Z","shell.execute_reply.started":"2024-08-29T06:05:27.904802Z","shell.execute_reply":"2024-08-29T06:05:27.909517Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\n# Iterate over dataset and process each item\nfor item in json_data:\n    attempts = 0\n    while attempts < 3:\n        try:\n            data = json.loads(item)\n            diff_id = data['diff_id']\n            diff = data['diff']\n            result = remove_between_identifiers(diff, 'mmm a', '<nl>')\n            diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\n            msg = data['msg']\n\n            words = msg.split()\n            msg_list = [to_Underline(word) if is_camel_case(word) else word for word in words]\n            msg = ' '.join(msg_list)\n\n            # Generate commit message using Mistral 7B\n            response = nlp_pipeline(\n                f\"{diff}\\nPlease write a commit message that contains only one simple sentence for the above code change.\\n\",\n                max_new_tokens=50\n            )[0]['generated_text']\n\n            msgGPT = response.strip().split('\\n')[-1].strip()\n\n            # Calculate metrics\n            bleu_score = round(calculate_bleu(msg, msgGPT), 2)\n            rouge_l_score = round(calculate_rouge_l(msg, msgGPT)['f'], 2)\n            meteor_score = round(calculate_meteor(msg, msgGPT), 2)\n\n            # Merge and save results with cleaner format\n            merged_data = {\n                \"diff_id\": diff_id,\n                \"msg\": msg,\n                \"msgGPT\": msgGPT,\n                \"METEOR Score\": f\"{meteor_score}\",\n                \"BLEU Score\": f\"{bleu_score}\",\n                \"ROUGE-L Score\": f\"{rouge_l_score}\"\n            }\n\n            with open(output_file, 'a', encoding='UTF-8') as f:\n                json.dump(merged_data, f)\n                f.write('\\n')\n            time.sleep(2)\n            break\n\n        except Exception as e:\n            traceback.print_exc()\n            attempts += 1\n            if attempts == 3:\n                print(f\"Failed to process item after 3 attempts: {item}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:23:19.870197Z","iopub.execute_input":"2024-08-18T16:23:19.870835Z","iopub.status.idle":"2024-08-18T16:25:06.894074Z","shell.execute_reply.started":"2024-08-18T16:23:19.870805Z","shell.execute_reply":"2024-08-18T16:25:06.893268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the code difference and human reference\ncode_diff = \"\"\"@@ -62,7 +62,7 @@ public class MongoDBStorageProvider implements IStorageProvider\nprivate String host = \"localhost\"; private int port = 27017;\nprivate String password;\n- private String name;\n+ private String name = \"default\";\n\"\"\"\n\nhuman_reference = \"Fixing mongo storage provider name.\"\n\n# Refined prompt to avoid confusion\nprompt = (\n    f\"{code_diff}\\nPlease generate a concise and meaningful commit message for the above code change.\\n\"\n)\nresponse = nlp_pipeline(\n                  prompt,\n                max_new_tokens=50\n            )[0]['generated_text']\n\n\ngenerated_msg = response.strip().split('\\n')[-1].strip()\n\n# Calculate metrics\nbleu_score = round(calculate_bleu(human_reference, generated_msg), 2)\nrouge_l_score = round(calculate_rouge_l(human_reference, generated_msg)['f'], 2)\nmeteor_score = round(calculate_meteor(human_reference, generated_msg), 2)\n\n# Print the results\nprint(f\"Original Message: {human_reference}\")\nprint(f\"Generated Message: {generated_msg}\")\nprint(f\"METEOR Score: {meteor_score}\")\nprint(f\"BLEU Score: {bleu_score}\")\nprint(f\"ROUGE-L Score: {rouge_l_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T06:12:58.431828Z","iopub.execute_input":"2024-08-29T06:12:58.43228Z","iopub.status.idle":"2024-08-29T06:13:14.730821Z","shell.execute_reply.started":"2024-08-29T06:12:58.432248Z","shell.execute_reply":"2024-08-29T06:13:14.729892Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Original Message: Fixing mongo storage provider name.\nGenerated Message: * Add default name to\nMETEOR Score: 5.87\nBLEU Score: 0.13\nROUGE-L Score: 0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}