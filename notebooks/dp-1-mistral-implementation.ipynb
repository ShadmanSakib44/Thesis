{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9197834,"sourceType":"datasetVersion","datasetId":5560773}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shadman200042144/dp-1-mistral-implementation?scriptVersionId=193693514\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install langchain-community langchain-core\n!pip install -U bitsandbytes\n!pip install accelerate\n!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:21:45.061571Z","iopub.execute_input":"2024-08-18T16:21:45.062491Z","iopub.status.idle":"2024-08-18T16:22:44.221937Z","shell.execute_reply.started":"2024-08-18T16:21:45.062453Z","shell.execute_reply":"2024-08-18T16:22:44.221057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\nfrom langchain import HuggingFacePipeline\nfrom langchain import PromptTemplate, LLMChain\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:22:51.97723Z","iopub.execute_input":"2024-08-18T16:22:51.977597Z","iopub.status.idle":"2024-08-18T16:23:10.81622Z","shell.execute_reply.started":"2024-08-18T16:22:51.97756Z","shell.execute_reply":"2024-08-18T16:23:10.815378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\nhuggingface_hub.login()\n#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","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:23:10.817653Z","iopub.execute_input":"2024-08-18T16:23:10.818217Z","iopub.status.idle":"2024-08-18T16:23:10.844438Z","shell.execute_reply.started":"2024-08-18T16:23:10.818189Z","shell.execute_reply":"2024-08-18T16:23:10.843468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:23:17.051036Z","iopub.execute_input":"2024-08-18T16:23:17.051869Z","iopub.status.idle":"2024-08-18T16:23:17.926025Z","shell.execute_reply.started":"2024-08-18T16:23:17.051836Z","shell.execute_reply":"2024-08-18T16:23:17.925133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nimport json\nimport re\nimport time\nimport traceback\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Configure BitsAndBytes with CPU offloading for specific parts of the model\nbnb_config = BitsAndBytesConfig(\n    llm_int8_enable_fp32_cpu_offload=True,  # Enable FP32 offload to CPU\n    load_in_8bit=True,  # Enable 8-bit quantization\n    llm_int8_threshold=6.0  # Default threshold for mixed precision\n)\n# Load the Mistral 7B model and tokenizer with a custom device map\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Automatically distribute model layers across devices\n)\n\n# Create a pipeline for text generation\nnlp_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Define file paths\ninput_file = '/kaggle/input/java-dataset/java.jsonl'  # Your dataset\noutput_file = '/kaggle/working/javaOutput.jsonl' \n\n# Load dataset\nwith open(input_file, 'r', encoding='UTF-8') as f:\n    json_data = f.readlines()\n\n# Functions for evaluation metrics\ndef calculate_meteor(sentence1, sentence2):\n    vectorizer = CountVectorizer().fit([sentence1, sentence2])\n    sentence1_vector = vectorizer.transform([sentence1])\n    sentence2_vector = vectorizer.transform([sentence2])\n    similarity = cosine_similarity(sentence1_vector, sentence2_vector)[0][0]\n    score = 2 * similarity * len(sentence1) * len(sentence2) / (len(sentence1) + len(sentence2))\n    return score\n\ndef calculate_bleu(reference, translation):\n    bleu_score = sentence_bleu([reference], translation)\n    return bleu_score\n\ndef calculate_rouge_l(reference, translation):\n    rouge = Rouge()\n    rouge_l_score = rouge.get_scores(translation, reference, avg=True)['rouge-l']\n    return rouge_l_score\n\n# Preprocessing functions\ndef is_camel_case(s):\n    return s != s.lower() and s != s.upper() and \"_\" not in s\n\ndef to_Underline(x):\n    return re.sub('(?<=[a-z])[A-Z]|(?<!^)[A-Z](?=[a-z])', ' \\g<0>', x).lower()\n\ndef get_tokens(text):\n    tokens = nltk.word_tokenize(text)\n    if len(tokens) > 1024:\n        return ' '.join(tokens[:1024])\n    else:\n        return ' '.join(tokens)\n\ndef remove_between_identifiers(text, identifier_start, identifier_end):\n    pattern = f'(?<={identifier_start}).*?(?={identifier_end})'\n    result = re.sub(pattern, '', text)\n    result = result.replace(' . ', '.').replace('  ', ' ').replace(' = ', '=').replace(' ; ', ';')\n    return result\n\n# Initialize results file\ninitial_data = {\"diff_id\": 0, \"msg\": \"0\", \"msgGPT\": \"0\", \"METEOR Score\": \"0\", \"BLEU Score\": \"0\", \"ROUGE-L Score\": \"0\"}\nwith open(output_file, 'a', encoding='UTF-8') as f:\n    json.dump(initial_data, f)\n    f.write('\\n')\n\n# Iterate over dataset and process each item\nfor item in json_data:\n    attempts = 0\n    while attempts < 3:\n        try:\n            data = json.loads(item)\n            diff_id = data['diff_id']\n            diff = data['diff']\n            result = remove_between_identifiers(diff, 'mmm a', '<nl>')\n            diff = get_tokens(remove_between_identifiers(result, 'ppp b', '<nl>'))\n            msg = data['msg']\n\n            words = msg.split()\n            msg_list = [to_Underline(word) if is_camel_case(word) else word for word in words]\n            msg = ' '.join(msg_list)\n\n            # Generate commit message using Mistral 7B\n            response = nlp_pipeline(\n                f\"{diff}\\nPlease write a commit message that contains only one simple sentence for the above code change.\\n\",\n                max_new_tokens=50\n            )[0]['generated_text']\n\n            msgGPT = response.strip().split('\\n')[-1].strip()\n\n            # Calculate metrics\n            bleu_score = round(calculate_bleu(msg, msgGPT), 2)\n            rouge_l_score = round(calculate_rouge_l(msg, msgGPT)['f'], 2)\n            meteor_score = round(calculate_meteor(msg, msgGPT), 2)\n\n            # Merge and save results with cleaner format\n            merged_data = {\n                \"diff_id\": diff_id,\n                \"msg\": msg,\n                \"msgGPT\": msgGPT,\n                \"METEOR Score\": f\"{meteor_score}\",\n                \"BLEU Score\": f\"{bleu_score}\",\n                \"ROUGE-L Score\": f\"{rouge_l_score}\"\n            }\n\n            with open(output_file, 'a', encoding='UTF-8') as f:\n                json.dump(merged_data, f)\n                f.write('\\n')\n            time.sleep(2)\n            break\n\n        except Exception as e:\n            traceback.print_exc()\n            attempts += 1\n            if attempts == 3:\n                print(f\"Failed to process item after 3 attempts: {item}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T16:23:19.870197Z","iopub.execute_input":"2024-08-18T16:23:19.870835Z","iopub.status.idle":"2024-08-18T16:25:06.894074Z","shell.execute_reply.started":"2024-08-18T16:23:19.870805Z","shell.execute_reply":"2024-08-18T16:25:06.893268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}